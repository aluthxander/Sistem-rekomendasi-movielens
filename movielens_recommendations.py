# -*- coding: utf-8 -*-
"""movielens_recommendations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bftpCs7a-GGVwOUDINDuU_dZGlaAs6ig

# Movielens
Pada proyek ini saya membuat sistem rekomendasi content-based filtering dan collaborative filtering. Disini saya menggunakan Dataset Movielens. Dataset ini merupakan Dataset yang berisi 1.000.209 peringkat anonim dari sekitar 3.900 film
dibuat oleh 6.040 pengguna MovieLens yang bergabung dengan MovieLens pada tahun 2000. untuk lebih lengkapnya bisa dicek pada tautan dibawah ini:\
[MovieLens](https://www.kaggle.com/datasets/sherinclaudia/movielens)

## Download Dataset
Disini saya akan langsung melakukan download dari kaggle. dengan kode sebagai berikut:
"""

!pip install -q kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets list

!kaggle datasets download -d sherinclaudia/movielens

"""unzip file yang telah didownload"""

!unzip movielens.zip -d /content/

"""## Data Understanding
Pada tahap ini saya melakukan eksplorasi pada data untuk mendapatkan insight. disini saya hanya menggunakan library pandas untuk melakukannya.
"""

import pandas as pd

movies_cols = ['MovieID','Title','Genres']
ratings_cols = ['UserID','MovieID','Rating','Timestamp']
users_cols = ['UserID','Gender','Age','Occupation','Zip-code']

"""karena data yang diterima dalam bentuk **.dat** jadi perlu ditambahkan beberapa parameter seperti `sep`dan `encoding` (kadang jika tidak diberikan akan muncul error). karena pada datasetnya tidak mengandung header maka perlu ditambahkan header pada parameternya."""

movies=pd.read_csv('/content/movies.dat',sep='::',header=None,names=movies_cols, encoding='latin 1')
movies.head()

ratings=pd.read_csv('/content/ratings.dat',sep='::',header=None,names=ratings_cols, encoding='latin 1')
ratings.head()

users=pd.read_csv('/content/users.dat',sep='::',header=None,names=users_cols, encoding='latin 1')
users.head()

print(f'Ukuran Data Movies : {movies.shape}')
print(f'Ukuran Data ratings : {ratings.shape}')
print(f'Ukuran Data users : {users.shape}')

movies.info()

ratings.info()

"""Dari code diatas dapat diketahui ukuran data untuk **movies** adalah 3883 x 3 dan ratings adalah 1000209 x 4. ini ukuran data yang cukup besar. dan untuk mengetahui _unique_ pada masing-masing features rating, dapat menjalankan kode berikut:"""

users.info()

"""## Exploratory Data Analysis
### Univariate Exploratory Data Analysis
Pada proses ini saya akan mencari kode unik pada masing-masing fitur dengan menjalankan kode berikut:
"""

# Jumlah user
print('ratings dataset mempunyai', ratings['UserID'].nunique(), 'unique users')
# jumlah film
print('ratings dataset mempunyai', ratings['MovieID'].nunique(), 'unique movies')
# jumlah ratings
print('ratings dataset mempunyai', ratings['Rating'].nunique(), 'unique ratings')
# List dari unique ratings
print('unique ratings terdiri dari', sorted(ratings['Rating'].unique()))

print('jumlah kategori pada gender: ',len(users['Gender'].unique()))
print('kategori pada gender: ',users['Gender'].unique())

print('jumlah unik umur: ',len(users['Age'].unique()))
print('unik umur: ',sorted(users['Age'].unique()))

print('jumlah unik pekerjaan: ',len(users['Occupation'].unique()))
print('kategori pada Occupation: ',sorted(users['Occupation'].unique()))

"""
Selanjutnya saya akan melakukan Explorasi lebih dalam mengenai dataset `movieslens`. Agar dataset tidak mengalami perubahan saat membuat sistem rekomendasi nanti, jadi saya mencopy datanya ke variabel lain. """

movies2 = movies.copy()
movies2.head()

"""Untuk kode dibawah ini, saya memunculkan seluruh kategori pada features `Genres` """

genre_labels = set()
for s in movies2['Genres'].str.split('|').values:
    genre_labels = genre_labels.union(set(s))
print(len(genre_labels))
genre_labels

"""Fungsi ini akan mengitung jumlah film pada masing-masing kategori di features `Genres`"""

def countWord(dataset, ref_col, sensus):
    countKeywords = dict()
    for s in sensus: 
        countKeywords[s] = 0
    for sensus_keywords in dataset[ref_col].str.split('|'):        
        if type(sensus_keywords) == float and pd.isnull(sensus_keywords): 
            continue        
        for s in [s for s in sensus_keywords if s in sensus]: 
            if pd.notnull(s): 
                countKeywords[s] += 1
    keywordOccurences = []
    for k,v in countKeywords.items():
        keywordOccurences.append([k,v])
    keywordOccurences.sort(key = lambda x:x[1], reverse = True)
    return keywordOccurences, countKeywords

keyword_occurences, dum = countWord(movies2, 'Genres', genre_labels)
keyword_occurences

"""Dari kode diatas, dapat dianalisa bahwa banyak film dengan genre **Drama** dan **comedy** pada dataset ini.

### Bivariate Data Analysis
Sebelum menggunakan data untuk sistem rekomendasi. saya melakukan eksplorasi pada data terlebih dahulu. pertama-tama saya akan melakukan _merge_ pada dataset **movies**, **rating** dan users ke dalam variabel `merge_on_movieid`.
"""

merge_on_movieid = pd.merge(movies,ratings,on = 'MovieID').reset_index(drop = True)
merge_on_movieid.head()

merge_on_userid = pd.merge(merge_on_movieid, users, on = 'UserID').reset_index(drop = True)
merge_on_userid.head()

"""karena pada proses EDA ini saya tidak membutuhkan fitur Genres,Timestamp dan Zip-code. maka saya akan membuang fitur tersebut"""

master_data = merge_on_userid.drop(['Genres','Timestamp','Zip-code'],axis = 1)
master_data.head()

"""selanjutnya saya melakukan eksplorasi dengan menggunakan representasi visual (grafik/plot) dengan menggunakan library matplotlib dan seaborn"""

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import style

# user age distribution
style.use('ggplot')
plt.figure(figsize = (7,7))
master_data['Age'].value_counts().plot(kind = 'bar', color = 'blue')
plt.xlabel('Usia Users')
plt.ylabel('Jumlah Film')
plt.title('Grafik Distribusi Usia User')
plt.show()
plt.savefig('plot1')

"""dari grafik tersebut dapat diketahui bahwa, kabanyakan penonton film pada dataset ini berada pada usia 25-34 tahun."""

# user age distribution
plt.figure(figsize = (7,7))
master_data.groupby('Age')['Gender'].value_counts().unstack().plot(kind = 'bar',color = ['blue','orange'])
plt.xlabel('Usia User')
plt.ylabel('Jumlah Film')
plt.title('Grafik Distribusi Usia User : Male/Female')
plt.show()
plt.savefig('plot2')

"""dari grafik diatas dapat diketahui bahwa mayoritas penonton film pada dataset ini merupakan laki-laki.

# Content-Based Filtering

Disini saya membuat sistem rekomendasi content-base filtering. Untuk membuat sistem rekomendasi ini saya akan menggunakan `movies` dataset. saya akan memanfaatkan features `Genres` pada dataset untuk mendapatkan rekomendasi.

### Data Preprocessing
**Mencari missing value**\
pada tahap ini saya megecek apakah ada missing value atau tidak
"""

print(movies2.isna().sum())

"""Bisa dilihat, tidak ada missing value pada dataset ini, jadi saya akan ke tahap selanjutnya.\

### Data Preparation
pada tahap ini, saya mengubah features `Genres` ke dalam bentuk **string array**
"""

movies2['Genres'] = movies2.Genres.str.split('|')
movies2['Genres'] = movies2['Genres'].fillna("").astype('str')
movies2

"""### Model Development
**TF-IDF Vectorizer**\
Pada tahap ini saya akan membuat sistem rekomendasi content-based filtering. saya membangun sistem rekomendasi ini berdasarkan _genre_ film. disini saya menggunakan TF-IDF Vectorizer untuk menemukan representasi fitur penting dari setiap kategori film. untuk melakukannya saya menggunakan fungsi `TfidfVectorizer` pada library sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(movies2['Genres'])
tfidf_matrix.shape

"""**Similarity**\
Dalam menghitung drajat kesamaan (similarity degree) antar film, saya akan menggunakan `euclidean_distances` dari library sklearn.
"""

from sklearn.metrics.pairwise import euclidean_distances
euclidean_dist = euclidean_distances(tfidf_matrix, tfidf_matrix)
euclidean_dist[:4, :4]

"""Setelah menghitung kesamaan antar film. selanjutnya saya membuat fungsi `genre_recommendations` dengan satu parameter yaitu title(judul film yang ditonton). keluaran fungsi ini berupa rekomendasi film untuk pengguna sesuai genre film yang telah user tonton."""

titles = movies2['Title']
genres = movies2['Genres']
indices = pd.Series(movies2.index, index=movies2['Title'])

def genre_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(euclidean_dist[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:21]
    movie_indices = [i[0] for i in sim_scores]
    df = pd.DataFrame(titles.iloc[movie_indices])
    df['Genres'] = genres.iloc[movie_indices]
    return df

"""Selanjutnya saya akan mencoba sistem rekomendasi yang saya buat."""

idx = 5
movies2['Title'][idx], movies2['Genres'][idx]
print('film yang telah ditonton user',movies2['Title'][idx],' genre: ', movies2['Genres'][idx])

recomendation_user = genre_recommendations(movies2['Title'][idx])
recomendation_user.head(10)

"""dapat dilihat, sistem rekomendasi memunculkan 10 film. tapi dari 10 film tersebut terasa kurang meyakinkan dari genre nya. film yang telah ditonton user bergenre ['Action', 'Crime', 'Thriller'], sedangkan kebanyakan film yang direkomendasikan tidak sesuai. disini saya dapat memperbaikinya dengan mengganti metode similarity dengan menggunakan `cosine_similarity`. dan hasilnya didapatkan sebagai berikut :"""

from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim.shape

titles = movies2['Title']
genres = movies2['Genres']
indices = pd.Series(movies2.index, index=movies2['Title'])

def genre_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:21]
    movie_indices = [i[0] for i in sim_scores]
    df = pd.DataFrame(titles.iloc[movie_indices])
    df['Genres'] = genres.iloc[movie_indices]
    return df

idx = 5
movies2['Title'][idx], movies2['Genres'][idx]
print(movies2['Title'][idx], movies2['Genres'][idx])

recomendation_user = genre_recommendations(movies2['Title'][idx])
recomendation_user.head(10)

"""dapat dilihat. Bahwa film-film yang direkomendasikan sekarang jauh lebih masuk akal dari pada tadi.

# Collaborative Filtering
Pada tahap ini saya akan membuat sistem rekomendasi collaborative filtering. Disini saya akan memanfaatkan dataset **ratings**. Nantinya saya akan menggunakan keras pada framework tensorflow dalam membangun sistem rekomendasi ini. saya juga menggunakan library lain seperti numpy.

### Mencari Missing Value
"""

print(ratings.isna().sum())
print(ratings.isnull().sum())

"""dari kode diatas, dapat dilihat tidak ada _missing value_ atau nilai kosong pada data. sehingga saya langsung menuju ke tahap Data Preparation

## Data Preprocessing dan Preparation
pada tahap ini saya akan melakukan persiapan data sebelum data digunakan untuk melatih model. disini saya melakukan copy pada data ratings.
"""

import numpy as np

ratings3 = ratings.copy()
ratings3.info()

"""Pada tahap Preprocessing, saya melakukan persiapan data dengan encode fitur `UserID` dan `MovieID`"""

user_ids = ratings3['UserID'].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
print(user2user_encoded)
print(userencoded2user)

movie_ids = ratings3['MovieID'].unique().tolist()
movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}
print(movie2movie_encoded)
print(movie_encoded2movie)

"""Selanjutnya saya akan menggabungkan hasil encode tadi ke dalam dataset rating serta mengubah fitur `Rating` menjadi float."""

# mapping UserID dan MovieID ke dataframe user dan movie
ratings3["user"] = ratings3["UserID"].map(user2user_encoded)
ratings3["movie"] = ratings3["MovieID"].map(movie2movie_encoded)
# Mengubah rating menjadi nilai float
ratings3['Rating'] = ratings3['Rating'].values.astype(np.float32)
ratings3.head()

"""Mengecek beberapa data seperti jumlah user, jumlah film, dan mengubah rating menjadi float"""

num_users = len(user2user_encoded)
num_movies = len(movie_encoded2movie)

min_rating = min(ratings3['Rating'])
max_rating = max(ratings3['Rating'])

print(f'Jumlah User: {num_users}, Jumlah Film: {num_movies}, Rating Minimum: {min_rating}, Rating maximuR: {max_rating}')

"""### Pembagian Data untuk Data Training Dan Validasi
Pada tahap ini. saya akan membagi data saya menjadi data training dan validasi. disini saya menggunakan `train_test_split` pada library sklearn sehingga memudahkan saya dalam melakukannya. saya membagi data 80% training dan 20% testing.
"""

from sklearn.model_selection import train_test_split

X = ratings3[['user', 'movie']].values
# Membuat variabel y dengan melakukan normalisasi pada data terlebih dahulu
y = ratings3['Rating'].apply(lambda X: (X - min_rating) / (max_rating - min_rating)).values

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_val.shape, y_train.shape, y_val.shape

"""### Modeling
Untuk model sistem rekomendasi collaborative filtering, model menghitung skor kecocokan antara pengguna dan movie dengan teknik embedding. Pertama, melakukan proses embedding terhadap data user dan movie. Selanjutnya, lakukan operasi perkalian dot product antara embedding user dan movie. Selain itu, saya juga dapat menambahkan bias untuk setiap user dan resto. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid.
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_movies = num_movies
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
            mask_zero=True,
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.movie_embedding = layers.Embedding(
            num_movies,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
            mask_zero=True,
        )
        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)
        # Add all the components (including bias)
        x = dot_user_movie + user_bias + movie_bias
        # The sigmoid activation forces the rating to between 0 and 1
        return tf.nn.sigmoid(x)

EMBEDDING_SIZE = 32
model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation. 
Langkah berikutnya, mulailah proses training. 
"""

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 64,
    epochs = 5,
    verbose=1,
    validation_data = (X_val, y_val)
)

"""### Visualisasi Metrik
Untuk melihat visualisasi matriks. saya menggunakan maplotlib untuk melakukannya.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.savefig('plot3')

test_loss = model.evaluate(X_val, y_val)
print('\\nTest Loss: {}'.format(test_loss))

"""Dari proses ini, saya memperoleh nilai accuracy akhir sebesar sekitar 0.61 dan val loss pada data sebesar 0.24. Nilai tersebut cukup bagus untuk sistem rekomendasi.

### Mendapatkan Rekomendasi
Untuk mendapatkan rekomendasi film, pertama saya mengambil sampel user secara acak dan definisikan variabel movies_not_visited yang merupakan daftar film yang belum pernah ditonton oleh user. variabel tersebutlah yang akan direkomendasikan ke user.\
Sebelumnya, user telah memberikan rating ke beberapa film. rating-rating itu akan digunakan untuk rekomendasi film yang mungkin cocok untuk user tersebut. dan film yang direkomendasikan tersebut merupakan film yang belum pernah ditonton oleh users.
"""

print("Testing Model with 1 user")
movie_df = movies2
user_id = "new_user"
movies_watched_by_user = ratings3.sample(7)
# film yang belum pernah di lihat user
movies_not_watched = movie_df[~movie_df['MovieID'].isin(movies_watched_by_user.MovieID.values)]['MovieID'] 
movies_not_watched = list(
    set(movies_not_watched)
    .intersection(set(movie2movie_encoded.keys()))
)

movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]

user_movie_array = np.hstack(([[0]] * len(movies_not_watched), movies_not_watched))

"""Selanjutnya, saya menggunakan fungsi `model.predict()` untuk memberikan rekomendasi film kepada user sebanyak 5 film."""

ratings = model.predict(user_movie_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]

recommended_movie_ids = [
    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

print("Showing recommendations for user: {}".format(user_id))
print("====" * 9)
print("Film-film yang diberi rating tinggi oleh user")
print("----" * 8)
top_movies_user = (movies_watched_by_user.sort_values(by="Rating", ascending=False).head(5).MovieID.values)
movie_df_rows = movie_df[movie_df["MovieID"].isin(top_movies_user)]
for i, row in enumerate(movie_df_rows.itertuples()):
    i += 1
    print(i,'.',row.Title, ":", row.Genres)

print("----" * 8)
print("10 rekomendasi film untuk user")
print("----" * 8)
recommended_movies = movie_df[movie_df["MovieID"].isin(recommended_movie_ids)]
for i, row in enumerate(recommended_movies.itertuples()):
    i+= 1
    print(i,'.',row.Title, ":", row.Genres)